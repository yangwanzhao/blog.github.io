<!DOCTYPE html>
<html>
  <head>
  <title>Pytorch Chain Rule & Jacobian & Hessian – Younger. – 星星</title>

      <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="在学习pytorch tutorial的时候，看到numpy写的反向传播链式法则的代码，其中导数的求解是向量形式，有点没看明白，于是自己推了公式验证一下，在此记录，也算让自己留有关于求导在代码里是怎么表现的印象，而不是一味地依赖框架提供的自动求导的功能。

" />
    <meta property="og:description" content="在学习pytorch tutorial的时候，看到numpy写的反向传播链式法则的代码，其中导数的求解是向量形式，有点没看明白，于是自己推了公式验证一下，在此记录，也算让自己留有关于求导在代码里是怎么表现的印象，而不是一味地依赖框架提供的自动求导的功能。

" />
    
    <meta name="author" content="Younger." />

    
    <meta property="og:title" content="Pytorch Chain Rule & Jacobian & Hessian" />
    <meta property="twitter:title" content="Pytorch Chain Rule & Jacobian & Hessian" />
    
  <!-- Async font loading -->
<script>
  window.WebFontConfig = {
      custom: {
          families: ['Spoqa Han Sans:100,300,400,700'],
          urls: ['https://spoqa.github.io/spoqa-han-sans/css/SpoqaHanSans-kr.css']
      },
      timeout: 60000
  };
  (function(d) {
      var wf = d.createElement('script'), s = d.scripts[0];
      wf.src = 'https://ajax.googleapis.com/ajax/libs/webfont/1.5.18/webfont.js';
      s.parentNode.insertBefore(wf, s);
  })(document);
</script>

<!--adobe fonts-->
<script>
    (function(d) {
      var config = {
        kitId: 'vbt3gjx',
        scriptTimeout: 3000,
        async: true
      },
      h=d.documentElement,t=setTimeout(function(){h.className=h.className.replace(/\bwf-loading\b/g,"")+" wf-inactive";},config.scriptTimeout),tk=d.createElement("script"),f=false,s=d.getElementsByTagName("script")[0],a;h.className+=" wf-loading";tk.src='https://use.typekit.net/'+config.kitId+'.js';tk.async=true;tk.onload=tk.onreadystatechange=function(){a=this.readyState;if(f||a&&a!="complete"&&a!="loaded")return;f=true;clearTimeout(t);try{Typekit.load(config)}catch(e){}};s.parentNode.insertBefore(tk,s)
    })(document);
</script> 

  <!--[if lt IE 9]>
    <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->

  <link rel="stylesheet" type="text/css" href="/blog.github.io/style.css" />
  <link rel="alternate" type="application/rss+xml" title="Younger. - 星星" href="/blog.github.io/feed.xml" />
  <link rel="shortcut icon" href="https://raw.githubusercontent.com/FromEndWorld/LOFFER/master/images/favicon.png">
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script src="https://unpkg.com/feather-icons"></script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>

  <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->

</head>

  <body>
    <div class="wrapper-sidebar">
  <header class="sidebar clearfix">
    <div class="site-info">
      
        <a href="/blog.github.io/" class="site-avatar"><img src="https://yangwanzhao.github.io/blog.github.io/images/protrait3.png" /></a>
       
      <h1 class="site-name"><a href="/blog.github.io/">Younger.</a></h1>
      <p class="site-description">星星</p>
    </div>
  </header>

  <div class="navlist">
    <nav>
      
      
      <a href="/blog.github.io/">首页</a>
      
      
      
      <a href="/blog.github.io/about">关于</a>
      
      
      
      <a href="/blog.github.io/archive">归档</a>
      
      
      
      <a href="/blog.github.io/tags">标签</a>
      
      
    </nav>
  </div>

  <div class="wrapper-footer-desktop">
    <footer class="footer">
      <!-- Refer to https://codepen.io/ruandre/pen/howFi -->
<ul class="svg-icon">

  
  
  

  

  

  

  

  
  <li><a href="https://github.com/yangwanzhao" class="icon-13 github" title="GitHub"><svg viewBox="0 0 512 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg><!--[if lt IE 9]><em>GitHub</em><![endif]--></a></li>
  

  

  

  

  

  

  

  

  

</ul>



<p>Copyright (c) 2019 来自中世界</p>

    </footer>
  </div>
</div>

    
      <aside class="toc">
        <ul>
  <li><a href="#">Pytorch Chain Rule &amp; Jacobian &amp; Hessian</a>
    <ul>
      <li><a href="#grad_y_pred">grad_y_pred</a></li>
      <li><a href="#grad_w2">grad_w2</a></li>
      <li><a href="#grad_h_relu">grad_h_relu</a></li>
      <li><a href="#grad_h">grad_h</a></li>
      <li><a href="#grad_w1">grad_w1</a></li>
      <li><a href="#jacobian矩阵">Jacobian矩阵</a></li>
      <li><a href="#hessian矩阵">Hessian矩阵</a></li>
    </ul>
  </li>
</ul>
      </aside>
    

    <div id="main" role="main" class="wrapper-content">
      <div class="container">
        <article class="posts">
  <h1>Pytorch Chain Rule & Jacobian & Hessian</h1>

  <div clsss="meta">
    <span class="author">
      
    </span>

    <span class="date">
      2020-09-13
    </span>

    <ul class="tag">
      
      <li>
        <a href="http://localhost:4000/blog.github.io/tags#pytorch">
          pytorch
        </a>
      </li>
      
    </ul>
  </div>

  <div class="entry">
    <p>在学习pytorch tutorial的时候，看到numpy写的反向传播链式法则的代码，其中导数的求解是向量形式，有点没看明白，于是自己推了公式验证一下，在此记录，也算让自己留有关于求导在代码里是怎么表现的印象，而不是一味地依赖框架提供的自动求导的功能。</p>

<!-- more -->

<p>先上代码：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span>

<span class="c1"># similar with randn in torch
# which means the data is sampled from normal distribution
# rand will generate average distribution
</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span>

<span class="n">w1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-6</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w1</span><span class="p">)</span> <span class="c1"># (N, H)
</span>    <span class="n">h_relu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">h</span> <span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="c1"># (N, H)
</span>    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">h_relu</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w2</span><span class="p">)</span> <span class="c1"># (N, D_out)
</span>
    <span class="c1"># corresponding elements are subtracted to get the loss
</span>    <span class="c1"># total loss is the sum of the losses of each element
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

    <span class="n">grad_y_pred</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="c1"># (N, D_out)
</span>    <span class="n">grad_w2</span> <span class="o">=</span> <span class="n">h_relu</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">grad_y_pred</span><span class="p">)</span> <span class="c1"># (H, D_out)
</span>    <span class="n">grad_h_relu</span> <span class="o">=</span> <span class="n">grad_y_pred</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w2</span><span class="p">.</span><span class="n">T</span><span class="p">)</span> <span class="c1"># (N, H)
</span>    <span class="n">grad_h</span> <span class="o">=</span> <span class="n">grad_h_relu</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span> <span class="c1"># (N, H)
</span>    <span class="n">grad_h</span><span class="p">[</span><span class="n">h</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">grad_w1</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">grad_h</span><span class="p">)</span> <span class="c1"># (D_in, H)
</span>
    <span class="n">w1</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_w1</span>
    <span class="n">w2</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_w2</span>
</code></pre></div></div>

<p>这是一个很简单的神经网络，tutorial利用numpy实现了前向与后向算法，本文关注的重点在于如何用numpy进行向量式的求导，以下对五个导数分别进行说明。</p>

<p>现有 $ h_relu $， $W_2$ 方便查阅：</p>

\[h\_relu =
\left[
\begin{matrix}
 r_{11}      &amp;  r_{12}    &amp; \cdots &amp;  r_{1,H}  \\
 r_{21}      &amp;  r_{22}        &amp; \cdots    &amp;  r_{2,H}      \\
 \vdots      &amp;  \vdots	      &amp; \ddots    &amp; \vdots \\
 r_{N,1}     &amp;  r_{N,2}   &amp; \cdots &amp;  r_{N,H}    \\
\end{matrix}
\right]\]

\[W_2 = 
\left[
\begin{matrix}
 w^2_{11}      &amp;  w^2_{12}    &amp; \cdots &amp;  w^2_{1,D}  \\
 w^2_{21}      &amp;  w^2_{22}     &amp; \cdots    &amp;  w^2_{2,D}      \\
 \vdots      &amp;  \vdots	   &amp; \ddots    &amp; \vdots \\
 w^2_{H,1}     &amp;  w^2_{H,2}    &amp; \cdots &amp;  w^2_{H,D}    \\
\end{matrix}
\right]\]

<p>要明白的是，在写代码求导的时候，对于这样的向量操作，不要简单套用链式法则公式，而是应该逐个元素验证。比如 $grad_w2$ 的计算就涉及到 $grad_y_pred$ 中的每一个元素，因此才有tutorial中的结果。此外，以下变量都是向量或矩阵形式，其代表的是loss对其中每一个元素的导数。</p>

<h2 id="grad_y_pred">grad_y_pred</h2>

<p>标量loss其实是对矩阵 $y_pred-y$ 中每一个元素代表的误差的求和，即：</p>

\[loss = l_{11} + l_{12} + l_{13} + ... + loss_{N, D}\]

<p>其中N是样本数量，D是输出维度。</p>

<p>因此loss对 $y_pred$ 的导数在针对其中某一个变量的时候，其他变量均可忽视。易得:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">grad_y_pred</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<p>不用多说。</p>

<h2 id="grad_w2">grad_w2</h2>

<p>求loss对$W_2$的导数之前，先来看一下$y_pred$：</p>

\[y\_pred_{ij} = r_{i,1}*w^2_{1,j} + r_{i,2}*w^2_{2,j}+...+r_{i,H}*w^2_{H,j}\]

<p>loss对$W_2$的导数，其实是对每一个$w_{i,j}$的导数。根据链式法则，对$w_{i,j}$的导数等于其对 $y_pred$ 的导数 $grad_y_pred_{i,j}$ 乘以 $y_pred$ 对 $w_{i,j}$ 的导数。这里需要注意的是，考虑loss对 $w_{i,j}$ 的导数的时候，不能顺着之前计算的结果，从某一个 $grad_y_pred_{i,j}$ 出发计算对$w_{i,j}$的导数，因为其值与每一个 $grad_y_pred_{i,j}$  都有关系。例如考虑$w_{1,1}$，参数上式，与其有关的包括 $y_pred_{1,1}$，$y_pred_{2,1}$，…，$y_pred_{N,1}$。因此对于$w^2_{i,j}$的导数，有下式：
\(grad\_w2_{i,j} = grad\_y\_pred_{1,j}*r_{1,j}+grad\_y\_pred_{2,j}*r_{2,j}+...+grad\_y\_pred_{N,j}*r_{N,j}\)
即：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">grad_w2</span> <span class="o">=</span> <span class="n">h_relu</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">grad_y_pred</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="grad_h_relu">grad_h_relu</h2>

<p>同理，loss对 $h_relu$ 的导数等于loss对 $y_pred$ 的导数 $grad_y_pred_{i,j}$ 乘以 $y_pred$ 对  $h_relu$  的导数。参照grad_w2中的式子，在 $y_pred_{ij}$ 中找与 $r_{i,j}$ 有关的那些$y_pred$，易得（公式可右滑）：</p>

\[grad\_h\_relu_{i,j} = grad\_y\_pred_{i,1}*w^2_{i,1}+grad\_y\_pred_{i,2}*w^2_{i,2}+...+grad\_y\_pred_{i,H}*w^2_{i,H}\]

<p>即：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">grad_h_relu</span> <span class="o">=</span> <span class="n">grad_y_pred</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w2</span><span class="p">.</span><span class="n">T</span><span class="p">)</span> 
</code></pre></div></div>

<h2 id="grad_h">grad_h</h2>

<p>同理，loss对 $h$ 的导数等于loss对 $h_relu$ 的导数 $grad_h_relu$ 乘以 $h_relu$ 对$h$  的导数。由上可得loss对每一个 $h_relu$ 中的元素的导数，即矩阵 $grad_h_relu$。而 $h_relu$ 对 $h$ 的导数即$relu$函数的导数。</p>

<p>考虑$relu$函数，此时其自变量为 $h$ ，当其中某值 $h_{i,j}$ 大于等于0时，$relu$ 对其导数为1，因此 $grad_h_{i,j}$ 等于$grad_h_relu_{i,j}$乘以1；当 $h_{i,j}$  小于0时，$relu$ 为常数0，因此求导得0。故有：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">grad_h</span> <span class="o">=</span> <span class="n">grad_h_relu</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">grad_h</span><span class="p">[</span><span class="n">h</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div></div>

<h2 id="grad_w1">grad_w1</h2>

<p>最后，loss对 $W_1$ 的导数等于loss对 $h$ 的导数 $grad_h$ 乘以 $h$ 对 $W_1$  的导数。</p>

<p>现有$h$如下：</p>

\[h =
\left[
\begin{matrix}
 h_{11}      &amp;  h_{12}    &amp; \cdots &amp;  h_{1,H}  \\
 h_{21}      &amp;  h_{22}        &amp; \cdots    &amp;  h_{2,H}      \\
 \vdots      &amp;  \vdots	      &amp; \ddots    &amp; \vdots \\
 h_{N,1}     &amp;  h_{N,2}   &amp; \cdots &amp;  h_{N,H}    \\
\end{matrix}
\right]\]

<p>其中</p>

\[h_{i,j}=x_{i,1}*w_{1,j}+x_{i,2}*w_{2,j}+...+x_{i,I}*w_{I,j}\]

<p>考虑$W_1$的其中一个参数$w_{i,j}^1$，与之相关的$h$包括$h_{1,j}, h_{2,j}, …, h_{N,j}$。因此找到loss对它们各自的导数$grad_h_{1,j}, grad_h_{2,j}, …, grad_h_{N,j}$，从而易得：</p>

\[grad\_w1_{i,j} = grad\_h_{1,j}*x_{1,i}+grad\_h_{2,j}*x_{2,i}+...+grad\_h_{N,j}*x_{N,i}\]

<p>即：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>grad_w1 = x.T.dot(grad_h)
</code></pre></div></div>

<hr />

<h2 id="jacobian矩阵">Jacobian矩阵</h2>

<p>对Jacobian的大致理解是，对于基于向量的函数$\vec{y}=f(\vec{x})$，其对于$\vec x$的导数即为一个Jacobian矩阵：</p>

\[J=
\left[
\begin{matrix}
 \frac{\partial y_1}{\partial x_1}   &amp; \cdots &amp;  \frac{\partial y_1}{\partial x_n}  \\
\vdots      &amp; \ddots    &amp;  \vdots      \\
 \frac{\partial y_m}{\partial x_1}  &amp; \cdots &amp;   \frac{\partial y_m}{\partial x_n}    \\
\end{matrix}
\right]\]

<p>可以将该矩阵想象成仅含有输入层与输出层的神经网络，输入层是n个x神经元，输出层是m个y神经元，它们之间有m*n个connection，从而构成上述维度为（m,n）的Jacobian矩阵。该矩阵一方面表示了输出对输入的偏导，另一方面引出神经网络反向传播一种特殊的求导情况。通常意义上，神经网络的反向传播是求loss对每个可训练参数的偏导数，其中loss是标量scalar，此时可以用pytorch的loss.backward()进行自动求导。但有的时候loss可能是vector，无法直接backward求导，这里需要的其实就是loss中各元素对所有参数的偏导数，即上述Jacobian矩阵，为了仍可以使用pytorch的backward函数，这里给该函数一个与loss同维度的参数:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</code></pre></div></div>

<p>相当于多定义了一层总的 $loss_sum=0.1 * loss_1+1.0 * loss_2 + 0.0001 * loss_3$ ，然后对loss_sum进行求导。那么loss_sum对各参数的导数即为：</p>

\[J^T \cdot v=
\left[
\begin{matrix}
 \frac{\partial y_1}{\partial x_1}   &amp; \cdots &amp;  \frac{\partial y_m}{\partial x_1}  \\
\vdots      &amp; \ddots    &amp;  \vdots      \\
 \frac{\partial y_1}{\partial x_n}  &amp; \cdots &amp;   \frac{\partial y_m}{\partial x_n}    \\
\end{matrix}
\right]
\left[
\begin{matrix}
 \frac{\partial l}{\partial y_1}   \\
\vdots    \\
 \frac{\partial l}{\partial y_m}    \\
\end{matrix}
\right]=
\left[
\begin{matrix}
 \frac{\partial l}{\partial x_1}   \\
\vdots    \\
 \frac{\partial l}{\partial x_n}    \\
\end{matrix}
\right]\]

<p>这种Jacobian矩阵形式的求导方式，即给backward参数的求导方式，可以为没有scalar输出的神经网络提供外部的导数。</p>

<h2 id="hessian矩阵">Hessian矩阵</h2>

<p>Hessian矩阵是自变量为向量的实值函数的二阶偏导数组成的方阵，该实值函数如下：</p>

\[f(x_1,x_2,...,x_n)\]

<p>若该函数的所有二阶导都存在，那么其Hessian矩阵$H(f)$为：</p>

\[H(f) = 
\left[
\begin{matrix}
 \frac{\partial^2 f}{\partial x_1^2}       &amp;   \frac{\partial^2 f}{\partial x_1 \partial x_2}    &amp; \cdots &amp;  \frac{\partial^2 f}{\partial x_1 \partial x_n}  \\
 \frac{\partial^2 f}{\partial x_2 \partial x_1}       &amp;   \frac{\partial^2 f}{\partial x_2^2}    &amp; \cdots &amp;  \frac{\partial^2 f}{\partial x_2 \partial x_n}  \\
 \vdots      &amp;  \vdots	   &amp; \ddots    &amp; \vdots \\
 \frac{\partial^2 f}{\partial x_n \partial x_1}       &amp;   \frac{\partial^2 f}{\partial x_n \partial x_2}    &amp; \cdots &amp;  \frac{\partial^2 f}{\partial x_n^2}  \\
\end{matrix}
\right]\]

<p>Hessian矩阵主要被用于最优化。最优化问题其实是求解函数导数为0时方程的根，于是将$f$在某探索点处二阶泰勒展开，并利用迭代的方式求得方程的根，其中用到了Hessian矩阵。该方法即经典的牛顿法，更多细节有待补充。</p>

<p>（参考自：<a href="http://jacoxu.com/jacobian矩阵和hessian矩阵/">http://jacoxu.com/jacobian矩阵和hessian矩阵/</a>）</p>


  </div>

  
  
</article>

<div class="pagination">
  
    <span class="prev" >
      <a href="http://localhost:4000/blog.github.io/pytorch-code-block/">
        ← 上一篇
      </a>
    </span>
  
  
</div>
      </div>
    </div>

    

  </body>

  
  <script>
    document.getElementById("main").classList.add("withtoc");
  </script>
  

  <div class="wrapper-footer-mobile">
    <footer class="footer">
      <!-- Refer to https://codepen.io/ruandre/pen/howFi -->
<ul class="svg-icon">

  
  
  

  

  

  

  

  
  <li><a href="https://github.com/yangwanzhao" class="icon-13 github" title="GitHub"><svg viewBox="0 0 512 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg><!--[if lt IE 9]><em>GitHub</em><![endif]--></a></li>
  

  

  

  

  

  

  

  

  

</ul>



<p>Copyright (c) 2019 来自中世界</p>

    </footer>


</html>
